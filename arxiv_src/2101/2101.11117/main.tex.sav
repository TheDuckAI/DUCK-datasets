\newcommand*{\BODKIM}{}%
% \newcommand*{\EXAMPLES}{}%

\ifdefined\BODKIM
\documentclass[conference,letterpaper]{IEEEtran}
%   \documentclass[journal,onecolumn,12pt,twoside]{IEEEtranTCOM}
% \documentclass[article,twocolumn]{IEEEtran}

\else
\documentclass[journal]{IEEEtran}
\fi


%\documentclass[journal,draftcls,onecolumn,12pt,twoside]{IEEEtranTCOM}
\normalsize
%\makeatother
%\pagestyle{headings}
% adjust as needed
%\addtolength{\footskip}{0\baselineskip}
\addtolength{\textheight}{2\baselineskip}
%\addtolength{\textheight}{-5\baselineskip}

%\documentclass[10pt,conference]{IEEEtran}
%\documentclass[12pt]{article}
\usepackage{amssymb,amsmath}
\usepackage{bbding}
\usepackage{flushend,cuted}
%\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{float} % is used for \begin{figure}{H}
\usepackage{srcltx}
\usepackage{mathrsfs} % is required for \mathscr alwhichphabet.
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{enumitem}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother



\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{psfrag}
\usepackage{subfigure}

\usepackage{setspace}
\usepackage{multicol}
\usepackage[noadjust]{cite}
\usepackage{hyperref}

%\usepackage{stfloats}
%\usepackage{amssymb}
%\usepackage{fancybox}
%\usepackage{latexsym}

\usepackage[]{units} % for nicefrac
\usepackage{url} % for URL bib
\usepackage[dvips]{color}
\usepackage{verbatim} % for comment environment
%\usepackage{hyperref} % for hyperlinks of references
%\usepackage{mathtools} \mathtoolsset{showonlyreftrue} % for removing the number from non-referenced equations
\usepackage{cite} % for [1]-[3]
\usepackage{ifthen} % for TpX
\usepackage{ifpdf} % for TpX
\usepackage{soul}
\usepackage{mathtools}

% -------------------------------------------------------------------
% Define the "Theorems" style
% -------------------------------------------------------------------
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
%\newtheorem{algorithm}{Algorithm}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{question}{Question}
\newtheorem{conjecture}{Conjecture}

\newcommand\given[1][]{\:#1\vert\:}


\makeatletter
\def\widebreve#1{\mathop{\vbox{\m@th\ialign{##\crcr\noalign{\kern3\p@}%
      \brevefill\crcr\noalign{\kern3\p@\nointerlineskip}%
      $\hfil\displaystyle{#1}\hfil$\crcr}}}\limits}

\def\brevefill{$\m@th \setbox\z@\hbox{$\braceld$}%
  \bracelu\leaders\vrule \@height\ht\z@ \@depth\z@\hfill\braceru$}
\renewcommand*\env@matrix[1][*\c@Ma*matrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatletter

\DeclareRobustCommand{\Nt}[1][Nt]{\ensuremath {N}}
\DeclareRobustCommand{\PrecMat}[1][Nt]{\ensuremath{G}}
\DeclareRobustCommand{\PowerMat}[1][Nt]{\ensuremath{P}}
\ifdefined\BODKIM
\newcommand\figSize{0.92}
\else
\newcommand\figSize{0.92}
\fi

\input{eli_macros.tex}
\input{rv_defs.tex}

%   %\documentclass[article,twocolumn]{IEEEtran}
% \documentclass[conference]{IEEEtran}
% %\documentclass[journal,draftcls,onecolumn,12pt,twoside]{IEEEtranTCOM}
% %\usepackage{blindtext, graphicx}
% \usepackage{cite}
% \usepackage{graphicx}
% \usepackage{amssymb,amsmath}
% %\usepackage{amsthm}
% \usepackage{amsfonts}
% \usepackage{float} % is used for \begin{figure}{H}
% \usepackage{srcltx}
% \usepackage{mathrsfs} % is required for \mathscr alphabet.
% \usepackage{multirow}
% \usepackage{bbm}
% \usepackage{cuted}
% \setcounter{Ma*matrixCols}{64}

% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage{psfrag}
% %\usepackage{subfigure}
% %\usepackage{subcaption}

% \usepackage{setspace}
% %\usepackage{amssymb}
% %\usepackage{fancybox}
% %\usepackage{latexsym}

% \usepackage[]{units} % for nicefrac
% \usepackage{url} % for URL bib
% \usepackage[dvips]{color}
% \usepackage{verbatim} % for comment environment

% \newcommand{\svv}[1]{\mathbf{#1}}
% \DeclareRobustCommand{\Nt}[1][Nt]{\ensuremath {N_t}}
% \DeclareRobustCommand{\alNt}[1][Nt]{\alpha(\Nt)}
% \DeclareRobustCommand{\aNorm}[1][aNorm]{\ensuremath {\|{\bf a}\|}}

\newcommand{\SNR}{\text{$\mathsf{SNR}$}}
% \DeclareMathOperator{\Tr}{Tr}

% \newtheorem{claim}{Claim}
% \newtheorem{lemma}{Lemma}
% \newtheorem{corollary}{Corollary}
% \newtheorem{theorem}{Theorem}
% \newtheorem{definition}{Definition}
% \newtheorem{algorithm}{Algorithm}
% \newtheorem{proposition}{Proposition}
% \newtheorem{example}{Example}
% \newtheorem{remark}{Remark}
% \newtheorem{question}{Question}
% \newtheorem{conjecture}{Conjecture}

% \newcommand{\diag}{\mathop{\mathrm{diag}}}

% %\input{eli_macros.tex}
\DeclareRobustCommand{\prob}[1][{\rm Pr}]{\ensuremath {{#1}}}
% \DeclareRobustCommand{\genGam}[1][\beta]{\ensuremath {{#1}}}

% \input{eli_macros.tex}

% % correct bad hyphenation here
% \hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\allowdisplaybreaks
%
% paper title
% can use linebreaks \\ within to get better formatting as desired

\title{On the Importance of Asymmetry and Monotonicity Constraints in Maximal Correlation Analysis}
% \author{
% \IEEEauthorblockN{Elad Domanovitz and Uri Erez}
% \\
% \IEEEauthorblockA{%Dept. of EE-Systems,
% %TAU\\
% Dept. EE-Systems, Tel Aviv University, Israel
% }
% \thanks{The work of \mathbb{E}. Domanovitz and U. Erez was supported in part by the Israel Science Foundation under Grant No. 1956/17.}
% \thanks{The material in this paper was presented in part at the 2018 IEEE
% International Symposium on Information Theory.}
% }
% % \author{Elad Domanovitz and Uri Erez

% % \thanks{The work of \mathbb{E}. Domanovitz and U. Erez was supported in part by the Israel Science Foundation under Grant No. 1956/17.}
% % %\thanks{The material in this paper was presented in part at the 2016 IEEE
% % %International Symposium on Information Theory, Barcelona.}
% % \thanks{\mathbb{E}. Domanovitz and U. Erez are with the Department of Electrical Engineering -- Systems, Tel Aviv University, Tel Aviv, Israel (email: domanovi,uri@eng.tau.ac.il).}
% % }
\author{
\IEEEauthorblockN{Elad Domanovitz and Uri Erez}
%\\
\IEEEauthorblockA{
Dept. EE-Systems \\
Tel Aviv University, Israel
}
% \author{Elad Domanovitz and Uri Erez%, \emph{Member, IEEE}
% \thanks{The work of \mathbb{E}. Domanovitz and U. Erez was supported in part by the Israel Science Foundation under Grant No. 1956/17.}
% % \thanks{The material in this paper was presented in part at the 2016 IEEE
% % International Symposium on Information Theory, Barcelona.}
% \thanks{\mathbb{E}. Domanovitz and U. Erez are with the Department of Electrical Engineering -- Systems, Tel Aviv University, Tel Aviv, Israel (email: domanovi,uri@eng.tau.ac.il).}
}
\maketitle
% \date{September 2017}





\begin{abstract}
% THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD.
The  maximal correlation coefficient is a well-established generalization of the Pearson correlation coefficient for  measuring non-linear dependence between random variables. It is appealing from a theoretical standpoint, satisfying R\'enyi's axioms for a measure of dependence. It is also attractive from a computational point of view due to the celebrated alternating conditional expectation algorithm, allowing to compute its empirical version  directly from observed data. Nevertheless, from the outset, it was recognized that the maximal correlation coefficient suffers from some fundamental deficiencies, limiting its usefulness as an indicator of estimation quality.
Another well-known measure of dependence is the  correlation ratio which also suffers from some drawbacks.
Specifically, the maximal correlation coefficient equals one too easily whereas the correlation ratio equals zero too easily.
The present work recounts some attempts that have been made in the past to alter the definition of the  maximal correlation coefficient in order to overcome its weaknesses and then proceeds to suggest a natural  variant of the maximal correlation coefficient as well as a modified conditional expectation algorithm to compute it.
The proposed dependence measure at the same time resolves the major weakness of the correlation ratio measure and may be viewed as a bridge between these two classical measures.
\end{abstract}

\section{Introduction}
 Pearson's correlation coefficient is a measure indicating how well one can
approximate (estimate in an average least squares sense) a (response) random variable $Y$ as a linear (more precisely affine) function  of a (predictor/observed) random variable $X$, i.e., as $Y=aX+b$.\footnote{We assume that the random variables $X$ and $Y$ have finite variance.}
The coefficient is given by
\begin{align}
    \rho(X \leftrightarrow Y)=\frac{\rm{Cov}(X,Y)}{\sqrt{\var(X)} \sqrt{ \var(Y)}}.
\end{align}
The coefficient is symmetric in $X$ and $Y$ so it just as well measures how well one can approximate $X$ as a linear function of $Y$.

The correlation ratio of $Y$ on $X$, suggested by Pearson (see, e.g., \cite{cramer2016mathematical}), similarly measures how well one can approximate $Y$ as a general admissible function of $X$, i.e., as $Y=f(X)$.\footnote{We define a function $f(\cdot)$ to be admissible w.r.t. the random variable $X$ if it is a Borel-measurable real-valued functions such that
%$$ \var(f(X)) < \infty$
$\mathbb{E}[f(X)]=0$ and $\mathbb{E}[f^2(X)]\leq\infty$.}
%The problem of estimating a random   (response) variable from a vector of (predictor) random variable/s  is central to statistical signal processing. % with an obvious application to machine learning.
%An related problem is quantifying the relation or degree of dependence between the response and predictor/s. Two classical measures are the correlation coefficient and the correlation ratio. These directly arise from minimum mean square error (MSE) estimation principles, the former minimizing the MSE subject to the constraint that the estimator is linear, the latter allowing any estimation function to be used.
%Starting with the bivariate case, we denote the response variable by $Y$ and  the predictor variable by $X$. The correlation ratio, suggested by Pearson/Kolomogorv [to add] is defined as
Specifically, the correlation ratio of $Y$ on $X$ is given by
\begin{align}
    \theta(X \rightarrow Y)
    & =\sqrt{\frac{\var(\mathbb{E}[Y|X])}{\var(Y)}}
    \label{eq:corrRatioEq1}
    % \\
    % &=\sqrt{1-\frac{\mathbb{E}[\var(Y|X)]}{\var(Y)}}. \nonumber
\end{align}
The correlation ratio can also be expressed as
\begin{align}
    \theta(X \rightarrow  Y)=\sup_{f}\rho(f(X) \leftrightarrow Y)
\label{eq:corr_ratio}
\end{align}
%where $\rho$ is Pearson's correlation coefficient, and
where the supremum is taken over all (admissible) functions $f$ (see, e.g., \cite{renyi1959new}).
This measure is naturally nonsymmetric.

We note that one may equivalently say that the correlation ratio measures how well one can approximate
$Y$ as $Y=aX'+b$ for some admissible transformation of the random variable $X'=f(X)$. While perhaps seeming superfluous at this point, this view will prove useful when considering different generalizations of the correlation ratio to the case where the observations are a random vector.

Similarly, the Hirschfeld-Gebelein-R\'enyi maximal correlation coefficient \cite{hirschfeld1935connection,gebelein1941statistische,renyi1959measures} measures the maximal (Pearson) correlation that can be attained by transforming the pair $X,Y$ into random variables $X'=g(X)$ and $Y'=f(Y)$; that is, how well $X'=aY'+b$ holds in a mean squared error sense for some pair of (admissible) functions $f$ and $g$. More precisely,  the maximum correlation coefficient is defined as the supermum over all (admissible) functions $f,g$ of the correlation between $f(X)$ and $g(Y)$:
\begin{align}
   \rho^{**}_{\rm max}(X \leftrightarrow Y)=\sup_{g,f}\rho(f(X) \leftrightarrow g(Y)).
\end{align}
This measure is again symmetric by definition. We use the superscript  ``**'' to indicate that both functions (applied to the response and the predictor random variables) need not satisfy any restrictions beyond being admissible.

The maximal correlation coefficient has some very pleasing properties.
In particular, in \cite{renyi1959measures}, R\'enyi
%explored properties of dependence measures. R\'enyi's ambitious target was to characterize the strength of
put forth a set of seven axioms deemed natural to require of a  measure for dependence between
a pair of random variables. He further established that the maximal correlation coefficient satisfies the full set of axioms. Unlike the correlation ratio, the maximal correlation coefficient ``does not equal zero too easily''. Nonetheless, this comes at the price of ``equaling one too easily'' as exemplified below.\footnote{See also Footnote~3 in \cite{renyi1959measures}.}


%To that end R\'enyi proposed seven axioms for a measure of dependence,
It is important to note that one of the axioms requires symmetry.
R\'enyi's seminal work inspired substantial subsequent work aiming to identify other measures of dependence satisfying the set of axioms. We refer the reader to \cite{samuel2001correlation} for a survey of some of these.
%dy, and %showed that the maximal correlation coefficient, suggested in %\cite{gebelein1941statistische} satisfies all of the axioms he put %forth.

Another appealing trait of the maximal correlation coefficient, greatly contributing to its popularity, is its relation to the
 mean square error and hence to a Euclidean geometric framework. In particular, it is readily computable numerically via the alternating conditional expectation (ACE) algorithm of Breiman and Friedman \cite{breiman1985estimating}.
Moreover, and as recalled in the sequel, the ACE algorithm naturally extends to cover linear estimation of a (transformed) random variable from a component-wise transformed random vector.


Despite its elegance and it being amenable to computation, as noted above, the maximal correlation coefficient suffers from some significant deficiencies as was recognized  since its inception.
Specifically, the maximal correlation coefficient equals unity ``too easily''; see, e.g. \cite{hall1967characterizing} and \cite{kimeldorf1978monotone}.
Indeed, the maximal correlation coefficient can equal $1$ even when the pair of random variables is nearly independent (as also demonstrated below).

Several suggestions were proposed over the years to alter the measure so as to overcome these drawbacks. One important avenue calls for limiting the functions applied to both random variables to be monotone functions \cite{kimeldorf1978monotone,ramsay1988monotone}.
As we observe next,
%while this restriction indeed results in a more satisfying measure of dependence (in the sense of  being a meaningful indicator of  the %quality of estimation possible),
such a restriction is not restrictive enough when it comes to the transformation applied to the response variable and at the same time it is overly restrictive (in fact, unnecessary) when it comes to the predictor variable.  In respect  to the latter, it is worth quoting the incisive comments of  Hastie and Tibshirani  in \cite{hastie1988monotone}:

\emph{``A monotone restriction makes sense for a response transformation because it is necessary to allow predictions of the response from the estimated model. On the other hand, why restrict
 predictor transformations (such as for displacement and weight in the city gas consumption problem) to be monotone? Instead, why not leave them unrestricted and let the data suggest the shape of the relevant transformation?"}

The goal of the present paper is first to reiterate some of the known drawbacks of the correlation ratio and the maximal correlation coefficient.
%as well as to strengthen the arguments %that its definition should be modified.
In particular, we demonstrate that requiring that the transformation applied to response variable is monotonic is {\em almost} sufficient to resolve the drawbacks of the maximal correlation coefficient but that one must strengthen somewhat the required ``degree'' of monotonicity. Specifically, we introduce the notion of $\kappa$-monotonicity and we then argue in favor of constraining the transformation only of the response random variable to be $\kappa$-monotonic, leading to a proposed semi-$\kappa$-monotone maximal correlation measure.

We show that this measure does not suffer from  the drawbacks of neither the maximal correlation coefficient nor of those of the correlation ratio. Further, we demonstrate that both the correlation ratio and the suggestion of Hastie and Tibshirani can be viewed as extreme cases of the suggested measure.
The suggested measure satisfies a modified R\'enyi set of axioms that does not sacrifice any natural requirements with respect to capturing {\em both} full dependence and independence. This is achieved by  doing away with the symmetry requirement so that ``full dependence'' is directional. Since in the context of prediction/estimation symmetry should not be expected, this comes at no price.
%We further show that the

%The goal of the present paper is first to reiterate some of the known drawbacks of the maximal correlation coefficient as well as to strengthen the arguments that its definition should be modified. We then argue in favor of constraining the transformation only of the response random variable to be monotonic.

In addition, we modify accordingly the ACE algorithm and establish its convergence subject to adding an $\kappa$-monotonicity constraint on the transformation applied to the response variable.

\section{Shortcomings of the Correlation Ratio and Maximal  Correlation Coefficient and a Proposed Resolution}

As a simple example consider two variables which share only the least significant bit:
\begin{align}
X=C +\sum_{i=1}^N A_i 2^i,~Y=C +\sum_{i=1}^N B_i 2^i
\label{eq:example}
\end{align}
where ${A_i},{B_i},C$ are mutually independent random variables, all taking the value $0$ or $1$ with equal probability.
Clearly, applying  modulo $2$ to both random variables yields
a maximal correlation of $1$ which seems quite unsatisfactory if our goal is estimation subject to reasonable distortion metrics.
% \begin{remark}[All bits are not created equal]
% This example illustrates that when is comes to estimation of continuous random variables, it is important to incorporate a reasonable distortion measure in when defining dependence. Thus, other measures that are invariant to the significance of different bits such as mutual information are also  inappropriate for for measuring the ability to predict one random variable from another.
% \end{remark}
%Further, we note that for estimation purposes, these %functions are useless as there is no way to recover the %response variable from the predictor variable after %applying these functions.


Disconcerted from this behavior of the maximal correlation coefficient, Kimeldorf and Sampson \cite{kimeldorf1978monotone}
proposed to alter its definition in such a way that it attains the maximal value of $1$ only if the pair of random variables is
mutually completely dependent.\footnote{Following the work of Lancaster \cite{lancaster1963correlation}, two random variables are said to be mutually completely dependent if they are almost surely invertible functions of one another.}
%While it is obvious that if two variables are MCD their max %correlation is $1$ the converse is not true.
This motivated them to define a modified measure as
\begin{align}
    \rho^{mm}_{\rm max}(X \leftrightarrow Y)=\sup_{g,f}\rho(f(X) \leftrightarrow g(Y)).
    \label{eq:MonotoneDependence}
\end{align}
where $f$ and $g$ are not only admissible but also monotone functions.

The problem with adopting this definition in the context of estimation is the symmetric constraints it imposes on the two transformations. As the process of estimation/prediction (and more generally inference) is directional, if  the goal of the dependence measure is to characterize how well one can achieve the latter tasks, there is no apparent reason to impose any restriction on the transformation applied to the observed data.
Several works aiming to modify R\'enyi's axioms to reflect this asymmetry include \cite{hall1967characterizing}, \cite{joag19844} and \cite{li2016true}.


\begin{remark}[Discrete random variables]
While the emphasis in this paper is on continuous random variables, we note that symmetric measures are  also generally not appropriate for quantifying  the potential for predicting one discrete random variable from another.
%other between discrete random variables. Due to lack of space we refer the reader to Remark~1 in
For further details, see \cite{semiEpsMonoFullPaper:Domanovitz2019}.\footnote{In the discrete case, there is need to impose monotonicity.}
%is not required to ensure invertibility.
% For instance, a natural measure in this case is the minimal possible probability of error when predicting one from the other.
% Clearly, this measure is also not symmetric.
% While minimum error probability is related via  universal lower and upper bounds to the conditional entropy and mutual information (the latter being a symmetric measure), as shown in \cite{tebbe1968uncertainty} (Equations 5 and 6), the gap between the lower and upper bounds (keeping the probability of error fixed) grows unbounded with the cardinality of the random variables.
% This is yet another indication that symmetric measures are ill-suited for estimation/prediction purposes.
\end{remark}

Indeed, a natural and quite satisfying directional measure of dependence between random variables is the correlation ratio defined in \eqref{eq:corr_ratio}.
While R\'enyi objected to the correlation ratio due to  its asymmetric nature, as was noted in \cite{hall1967characterizing}, when our goal is asymmetric  (i.e., estimating $Y$ from $X$), there is no reason for requiring symmetry from the measure.

Nonetheless, in many cases one does not have strong grounds to assume a particular ``parameterization" of the desired (response) random variable
$Y$ which is to be estimated. Thus, not allowing to apply any transformation to the response variable, as is the case of the correlation ratio, may in certain cases be too restrictive.
In other words, in the absence of a preferred ``natural" parametrization of the response variable, one may consider choosing a strictly monotone transformation (change of variables) so as to make it  easier to estimate.

Another drawback of the correlation ratio is that it vanishes too easily. Specifically, two dependent random variables can have a correlation ratio of zero. In light of the considerations discussed, we advocate the following modification to the definition of the maximal correlation coefficient.

% \begin{remark}
% One may think of the above argument in favor of allowing a change of variables as somewhat  reminiscent of the use of Jeffrey's prior in the context of non-Bayesian parameter estimation. In the latter case, one can consider the goal as ensuring asymptotically minimax optimalilty of the estimator.
% \end{remark}


% In light of the considerations discussed, we advocate the following modification to the definition of the maximal correlation coefficient.



\begin{definition}
A function $g$ is said to be $\kappa$-increasing if for all $y_2 \geq y_1$:\footnote{This condition may be viewed as the ``complement'' of the Lipschitz condition.}
\begin{align}
    g(y_2)-g(y_1) \geq \kappa(y_2-y_1).
    \label{eq:eps_increasing}
\end{align}
\label{def:eps_increasing}
\end{definition}

\begin{definition}
The semi-$\kappa$-monotone maximal correlation measure is defined as
\begin{align}
    \rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=\sup_{g,f}\rho(f(X)\leftrightarrow g(Y) )
\end{align}
\label{def:semiMonotoneMeasure}
where $0<\kappa<1$ and the supremum is taken over all admissible functions $f(x)$ and  $\kappa$-increasing (and admissible) functions $g(y)$.
\end{definition}
\begin{remark}
Limiting $g$ to be an increasing function is done only to simplify notations. Equivalently,  $g$ can be limited to be $\kappa$-decreasing, which can be defined analogously to Definition~\ref{def:eps_increasing}.
\end{remark}
\begin{remark}
Limiting $g$ to be $\kappa$-increasing implies that, in particular, it is invertible, which is a natural requirement.
% as it ensures that no information is lost due to reparameterization.
The magnitude of $\kappa$ may be viewed as a means to regularize the reparameterization.
\end{remark}

% \begin{remark}
% Since our motivation is to estimate $Y$ from $X$, a more natural measure seems like taking the supermum over all admissible functions $f(x)$ and admissible and \emph{invertible} functions $g(y)$ (or equivalently admissible and \emph{strictly} monotone functions $g(y)$). As we show next, calculating this measure
% \end{remark}
% on the correlation between $g(Y)$ and $f(X)$, i.e.

% A monotone transformation can be viewed as re-parameterization of the data. The suggested measure can shed light on asking what is the optimal parameterization to use in estimation. While if a natural parameterization exists, obviously it should be used, in other cases, where such parameterization is not straight forward, the optimal (monotone) function applied on the response can be viewed as the optimal parameterization to result with the best estimation.


% Denote with $Y$ the response variable and with $x_1,\ldots,x_p$ the predictor variables. We denote with $\tilde{x}=\[x_1x_1,\ldots,x_p\]^H$. The max correlation is defined as supermum over measurable functions $g(Y),~f(\tilde{X})$ with finite positive variance on the correlation between $g(Y)$ and $f(\tilde{X})$, i.e.,
% \begin{align}
%     {\rm Max Corr}=\sup_{f,g}\rho(f(\tilde{X}),g(Y))
% \end{align}
% where $\rho$ is product-moment-correlation coefficient (pearson's correlation coefficient).

% \begin{remark}
% Usually, max corr is defined for the bi-variate case (i.e. $p=1$). [Need to add]
% \end{remark}

\subsection{The vector observation case}


Let   $\svv{X}=(X_1,\ldots,X_p)$ be a vector of predictor variables.
We may generalize the maximal correlation coefficient as
%and with $\tilde{X}=[X_1,\ldots,X_p]^H $.
% We may generalize the maximal correlation coefficient is defined as supermum over admissible functions $g(Y),~f(\tilde{X})$ with finite positive variance on the correlation between $g(Y)$ and $f(\tilde{X})$, i.e.,
 \begin{align}
     \rho^{**}_{\rm max}( \svv{X} \leftrightarrow Y)=\sup_{g,f}\rho(f(\svv{X}) \leftrightarrow g(Y))
 \end{align}
where the supremum is over all admissible functions.


Following Breiman and Friedman \cite{breiman1985estimating}, we may also consider a simplified (quasi-additive) relationship between $Y$ and $\svv{X}$
where we seek an optimal linear regression between a transformation of $Y$ and a component-wise non-linear transformation of the predictor random vector $\svv{X}$. Denote the fraction of the variance not explained by a regression of $f(Y)$ on $\sum_i f_i(X_i)$ as
\begin{align}
    e^2(g,f_1,\ldots,f_p)=\frac{\mathbb{E}\left[\left(g(Y)-\sum_i f_i(X_i)\right)^2\right]}{\mathbb{E} [g(Y)^2]}.
\end{align}
In \cite{breiman1985estimating} it was shown that optimal transformations $\{f_i\},g$ exist and that the ACE algorithm converges to these optimal transformations.



Going back to the rationale for requiring $\kappa$-monotonicity, one may object to the example \eqref{eq:example} as being artificial and argue that the maximum correlation coefficient merely captures whatever dependence there is between the random variables.
% In this respect, it is worthwhile to quote Breiman \cite{breiman1988monotone} (commenting on \cite{ramsay1988monotone}):
% \emph{``I only know of infrequent cases in which I would
%  insist on monotone transformations. Finding non-monotonicity can lead to interesting scientific discoveries. If the appropriate transformation is monotone,
%  then the fitted spline functions (or ACE transformations) will produce close to a monotonic transformation. So it is hard to see what there is to gain in the
%  imposition of monotonicity.''}
However, as exemplified in   Section~\ref{sec:numericalExample}, the problematic nature of the maximal correlation coefficient becomes more pronounced when considering the multi-variate case and so does the necessity of restricting the transformation of the response variable (only) to be $\kappa$-monotone.

% We demonstrate now that the problematic nature of the maximal correlation coefficient becomes pronounced when considering the multi-variate case and so does the necessity of restricting the transformation of the response variable (only) to be $\kappa$-monotone.

% Specifically, let us consider again the example of \eqref{eq:example}.
% Suppose that $Y$ and $X$ are as defined but that in addition to $X$, there is another slightly noisy observation of $Y$, say $\tilde{X}=Y+Z$ where the variance of $Z$ is small with respect to that of $Y$. Clearly, the maximal correlation coefficient will still equal $1$, and the observation $\tilde{X}$ will be discarded even though it could have allowed to estimate $Y$ with small distortion.
% Thus, in this example, the maximal correlation coefficient is maximized by perfectly estimating the least significant bit while doing away with the more significant bits even though nearly distortion-less reconstruction is possible.
% %with futile functions that cannot be used for estimation, even though some of the other variables might have a level of dependence (albeit lower) which can be used for a more meaningful estimation.
% See also Section~\ref{sec:numericalExample} below for a numerical
% example.


% It was noted, thus, that maximal correlation equals unity ``too easily'' (see, e.g. \cite{hall1967characterizing}). As a simple example consider two variables which share the least significant bit. In that case the max correlation will equal one (as the optimal functions will just take this bit and ignore all other bits) while obviously there is very little level of dependence between these variables.

% Further, we note that for estimation purposes, these functions are useless as there is no way to recover the response variable from the predictor variable after applying these functions. As highlighted in \cite{hall1967characterizing} a better name for a measure which meet all the Renyi's axioms is a a ``measure of relationship'' while when we discuss estimation we seek for a ``measure of dependence''.

Ramsay \cite{ramsay1988monotone} proposed a modification of the ACE algorithm by imposing monotonicity constraints on all transformations. As discussed above, there is no apparent reason to impose such a restriction on the transformation applied to the predictor variable and at the same time, the condition with respect to the response variable should be somewhat stronger. We formulate an ACE algorithm enforcing $\kappa$-monotonicity only on the transformation of the response variable and establish its convergence to a global maximum.


% functions $f(\tilde{X})$ and $g(Y)$. Although this is a step in the right direction, this suggestion does not capture the full merits of the problem. Again, since the original problem is not symmetric (in the sense that we want to estimate the response variable from the predictor variables and not vice versa), requiring that both functions will be monotone does not capture this lack of symmetry and seems like a harsh requirement. Also, as indicated in \cite{breiman1988monotone} imposing monotonicity constraints on all variables may result with losing substantial information on the relationship between the variables. However, reviewing it in the context of multi-variate problem, unrestricted measure can result with ``shading'' of the meaningful parameters by unmineangful ones as we demonstrate in Section ???.

% Some works suggested to modify Renyi's axioms to reflect this asymmetry. See e.g. \cite{hall1967characterizing} and \cite{li2016true}.


% We further show that the ACE algorithm can be modified to converge to to optimal transformation (while restricting the transformation of the response to a monotone transformation) and as we example next, this results with an improved measure that gives a much more meaningful measure for dependence.


\section{Semi-$\kappa$-monotone maximal correlation measure and modified R\'enyi axioms}


We follow the approach of Hall \cite{hall1967characterizing} in defining an asymmetric variant of the  R\'enyi axioms; more precisely, we adopt a slight variation on the somewhat stronger version formulated by Li \cite{li2016true}. However, unlike both of these works, when it comes to  putting forward a candidate dependence measure satisfying the modified axioms, we adopt the  approach suggested in  \cite{hastie1988monotone} and define a new maximal correlation measure where we restrict \emph{only} the function applied to the response variable to be $\kappa$-monotone.

% We adopt the modified  R\'enyi axioms that proposed in \cite{li2016true}:
Assume $r(X\rightarrow Y)$  is to measure the degree of dependence of $Y$ on $X$. Then it should satisfy the following:
% \renewcommand{\labelitemi}{$\blacksquare$}
\begin{enumerate}[label=(\alph*)]
\item \mbox{$r(X\rightarrow Y)$} is defined for all non-constant  random variables $X,Y$ having finite variance.\footnote{In \cite{li2016true}, the first axiom only requires that $r(X\rightarrow Y)$ be defined  for  \emph{continuous} random variables $X,Y$.
%We show that $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)$ holds for the more general axiom defined in \cite{renyi1959measures} and \cite{hall1967characterizing}.
}
% \begin{itemize}
%     \item Holds from the definition of $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)$.
% \end{itemize}
\item $r(X\rightarrow Y)$ may not be equal to $r(Y\rightarrow X)$.
% \begin{itemize}
%     \item Holds from the definition of $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)$.
% \end{itemize}
\item $0\leq r(X\rightarrow Y)\leq 1$.
% \begin{itemize}
%     \item Holds from the definition of $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)$.  .
% \end{itemize}
\item $r(X\rightarrow Y)=0$ if and only if $X,Y$ are independent.
% \begin{itemize}
% \item  If $X,Y$ are independent than obviously \mbox{$\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=0$}. To see the other direction we recall that in proving this axiom for $\rho^{**}_{\rm max}(X \leftrightarrow Y)$, monotone indicator functions are used (see, e.g. \cite{renyi1959measures}) hence the same argument holds for $\rho^{*m_{\kappa}}_{\rm max}(X \leftrightarrow Y)$.
% \end{itemize}
% \item $r(X\rightarrow Y)=1$ if and only if there exists a non-constant admissible function $g$ such that  $g(Y)=f(X)$ almost surely for some admissible function $f$.\footnote{Hui requires that the measure be $1$ only if $Y=f(X)$ almost surely for some function $f(X)$.}
\item $r(X\rightarrow Y)=1$ if and only if $Y=f(X)$ almost surely for some admissible function $f$.
% \item $r(X\rightarrow Y)=1$ if and only if there exists a non-constant admissible function $g$ such that  $Y=f(X)$ almost surely for some admissible function $f$.
% \begin{itemize}
% \item It follows from the definition of the semi-monotone norm that if $g(Y)=f(X)$ than $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=1$. To show that the opposite direction holds we recall that if pearson's correlation equal $1$ it follows that there is a perfect linear regression between $g(Y)$ and some $f(X)$ hence we have $g(Y)=af(X)+b$. Denoting $f'(X)=af(X)+b$ we get that if $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=1$ it follows that $g(Y)=f'(X)$.
% \end{itemize}
\item If $f$ is an admissible bijection on $\mathbb{R}$, then $r(f(X)\rightarrow Y)=r(X\rightarrow Y)$
% \begin{itemize}
% \item Since a Borel-measurable bijection $f$ is invertible, it is straightforward that \mbox{$\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=\rho^{*m_{\kappa}}_{\rm max}(f(X)\rightarrow Y)$}.
% \end{itemize}
\item If $X,Y$ are jointly normal with correlation coefficient $\rho$, then $r(X\rightarrow Y)=|\rho|$.\footnote{In \cite{li2016true}, the last axiom only requires that if $X,Y$ are jointly normal with correlation coefficient $\rho$, $r(X\rightarrow Y)$ is a strictly
increasing function of $|\rho|$.}
% \begin{itemize}
%     \item It is well know that when $X,Y$ are jointly normal with correlation coefficient $\rho$, $\rho^{**}_{\rm max}(X\rightarrow Y)=|\rho|$ (see, e.g., \cite{lancaster1957some}). Since this suggests that the maximal correlation is achieved while taking $g(Y)=Y$ and $f(x)=X$ or $f(x)=-X$, it follows that also $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=|\rho$ and as a result a strictly increasing function of $|\rho|$.
% \end{itemize}
\end{enumerate}

We next observe that for absolutely continuous (or discrete) distributions, the
semi-$\kappa$-monotone maximal correlation measure of Definition~\ref{def:semiMonotoneMeasure} satisfies the proposed axioms.

It is  readily verified that axioms (a), (b) and (c) hold for the semi-$\kappa$-monotone maximal correlation measure of Definition~\ref{def:semiMonotoneMeasure}.


To show that axiom (d) holds, we note that if $X,Y$ are independent, then obviously \mbox{$\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=0$}, as so is even \mbox{$\rho^{**}_{\rm max}(X\leftrightarrow Y)$}. As for the other direction, we first note that it suffices to consider the case where the correlation ratio equals $0$ and $X,Y$ are dependent. Since the correlation ratio is $0$, it follows
% that $\mathbb{E}[Y|X=x]=const$ almost surely.
%Assume, without loss of generality, that $\mathbb{E}[Y]=\mathbb{E}[X]=0$.
%Since the correlation ratio is defined as the supremum of Peason's correlation over all admissible $f(X)$, having the correlation ratio equals zero means it equals zero for \emph{all} admissible functions.
from \eqref{eq:corrRatioEq1} that $\mathbb{E}[Y|X] \equiv const$ (in the mean square sense).
%and therefore
% $\mathbb{E}[Y|X=x]=0$ as well.
We may break the symmetry of $g(y)=y$ by defining, e.g.,
 \begin{align}
g_a(y)=\begin{cases}
y~~~y\geq a \\
\kappa y~~y<a
\end{cases}.
 \end{align}
 Consider two values of $x_1$ and $x_2$ for which
 $p(y|x_i)$ are not identical, as must exist by the assumption of dependence.
Let $a$  be a value such that
 \begin{align}
     \int^a p(y|x_1)ydy \neq  \int^a p(y|x_2)ydy.
 \end{align}
Without loss of generality, we may assume that the left hand side is smaller than the right hand side (we may rename $x_1$ and $x_2$).
Recalling that $\kappa<1$, it follows that
 \begin{align}
     \int p(y|x_1)g_a(y)dy > \int p(y|x_2)g_a(y)dy
 \end{align}
Thus,
$$\mathbb{E}[g_a(Y)|X=x_1] \neq \mathbb{E}[g_a(Y)|X=x_2]$$
and hence the correlation ratio between $Y'=g_a(Y)$ and $X$ is non-zero, giving a lower bound to the semi-$\kappa$-monotone maximal correlation measure between $X$ and $Y$.

% Calculating the correlation ratio means that $g(Y)=Y$. This means that $g(Y)$ is an odd function (since $g(-Y)=-g(Y)$). Therefore we have $\mathbb{E}[Yf(X)|X]=0$ for any $X$. We note then when computing the semi-$\kappa$-monotone maximal correlation, we have $\kappa<1$ hence $g(Y)$ is ``allowed'' to be a more general function (not necessarily an odd function), therefore it can be readily verified that in case $X,Y$ are dependant, there exist $\kappa$-monotone function $g(Y)$ and admissible function $f(X)$ for which $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)>0$.


%To see the other direction, we recall that in proving this axiom for $\rho^{**}_{\rm max}(X \leftrightarrow Y)$, monotone indicator functions are used (see, e.g. \cite{renyi1959measures}). Hence, the same argument holds without change for $\rho^{*m_{\kappa}}_{\rm max}(X \leftrightarrow Y)$.

Axiom (e) holds since for the semi-$\kappa$-monotone maximal correlation measure, not only is  $g$ invertible, but also $g^{-1}(Y)$ has finite variance (since $\kappa$ is strictly positive); hence, $g^{-1}(f(X))$ is admissible. Axiom (f) trivially holds. Axiom (g) holds since  for jointly normal $X,Y$ with coefficient $\rho$, we have \mbox{$\rho^{**}_{\rm max}(X\leftrightarrow Y)=|\rho|$} (see, e.g., \cite{lancaster1957some}). More details are given in \cite{semiEpsMonoFullPaper:Domanovitz2019}.

% To show that axiom (e) holds, we note that by definition if $Y=f(X)$ (almost surely), then $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=1$. To show that the opposite direction holds, we recall that if $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=1$, then by the properties of Pearson's correlation coefficient, there is a \emph{perfect} linear regression between $g(Y)$ and $f'(X)$ ($g,f'$ being the maximizing functions of the measure). Hence we have $g(Y)=af'(X)+b$ where $g$ is an increasing function with slope greater than $\kappa$. Since $g$ is invertible, we have $Y=g^{-1}(af'(X)+b)$. Denoting $f(X)=g^{-1}(af'(X)+b)$, we note that if $f'$ is admissible, then so is $f$. Hence, $Y=f(X)$ almost surely.
% where $g$ is monotone and admissible. %It follows that...

% Axiom (f) trivially holds.
% %since a Borel-measurable bijection $f$ is an invertible function. Therefore, it is straightforward that \mbox{$\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)=\rho^{*m_{\kappa}}_{\rm max}(f(X)\rightarrow Y)$}.
% To show that axiom (g) holds, we recall that it is well know that when $X,Y$ are jointly normal with correlation coefficient $\rho$, then \mbox{$\rho^{**}_{\rm max}(X\rightarrow Y)=|\rho|$} (see, e.g., \cite{lancaster1957some}). Since this implies that the maximal correlation is achieved taking $g(y)=y$ (i.e., a monotone function) and $f(x)=x$ or $f(x)=-x$, it follows that
% \begin{align}
%     \rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)&=\rho^{**}_{\rm max}(X\rightarrow Y) \nonumber \\
% &=|\rho|.
% \end{align}
\begin{remark}
We note that the correlation ratio, defined in \eqref{eq:corr_ratio},  satisfies all of the modified axioms except for the ``only if'' part of axiom (d).
\end{remark}
% \begin{remark}
% We note that one may define other dependence measures satisfying the modified R\'enyi axioms, most notably via the theory of copulas; see \cite{li2016true}. Nonetheless, we believe that the proposed
% %semi-$\kappa$-monotone maximal correlation
% measure has the advantage of being closely tied to linear regression methods and geometric considerations.
% \end{remark}
% \begin{remark}
% This set of axioms is almost identical to the set of axioms suggested in \cite{hall1967characterizing}. The only difference is in axiom (e) where \cite{li2016true} (and also \cite{renyi1959measures}) requires that $r(X\rightarrow Y)=1$ \emph{if and only if} $Y=f(X)$ almost surely for a Borel-measurable $f$ while \cite{hall1967characterizing} asked that $r(X\rightarrow Y)=1$ only \emph{if} $Y=f(X)$. Since $\rho^{*m_{\kappa}}_{\rm max}(X \leftrightarrow Y)$ holds for the stronger requirement of \cite{li2016true}, it can be shown that is meets all axioms suggested in \cite{hall1967characterizing}.
% \end{remark}

\section{Modified ACE Algorithm}
\label{sec:modACE}
We  now present a modification of the ACE algorithm to compute %\eqref{def:semiMonotoneMeasure}
the semi-$\kappa$-monotone maximal correlation measure $\rho^{*m_{\kappa}}_{\rm max}(X\rightarrow Y)$ and show that the algorithm converges to the optimal transformations.
We then generalize the algorithm to the quasi-additive multi-variate scenario.

\subsection{Single-variable predictor}

% The class of functions that we will consapplied on the predictor variables are the ones defined in \cite{breiman1985estimating} (to be applied on both the response and predictor variable). We start by recalling these functions.

% Given a of random variable $X$, a transformation is defined by a set of admissible functions $f$ defined on this random variable such that
% \begin{align}
%     \mathbb{E}[f(X)]=0~~\mathbb{E}[f^2(X)]\leq\infty.
%     \label{eq:aceFunctions}
% \end{align}

Following in the footsteps of
\cite{breiman1985estimating}, recall that the space of all random variables with finite variance is a Hilbert space, which we denote  $\mathcal{H}_2$, with the usual definition of the inner product \mbox{$<X,Y>=\mathbb{E}[X Y]$}, for $X,Y\in\mathcal{H}_2$.
Since applying an admissible function $f$
%defined in \eqref{eq:aceFunctions}
results in a random variable with  finite variance, we may define the subspace $\mathcal{H}_2(X)$ as the set of all random variables that correspond to an admissible function of $X$.

Similarly, the set of all  admissible functions of $Y$ is also a subspace of $\mathcal{H}_2$, which we denote by $\mathcal{H}_2(Y)$.
Now, if we limit the functions applied to $Y$ to be $\kappa$-increasing, we obtain a closed and convex subset of the Hilbert space $\mathcal{H}_2(Y)$. We denote this set by
$\mathcal{M_{\kappa}}(Y)$.



% This verified that the resulting set of functions is a closed convex set. To be more concrete, we denote with $\mathcal{H}_2^m(Y)$.
% % is also a Hilbert space with the inner product $<f'(X),f(X)>=\mathbb{E}[f'(X)\cdot f(X)]$. To be more concrete, $\mathcal{H}_2(X)$ is the set of all admissible functions $f$ such that $\mathbb{E}[f(X)]=0$, $\mathbb{E}[f^2(X)]\leq\infty$ with $<f'(X),f(X)>=\mathbb{E}[f'(X)\cdot f(X)]$.

% To get the new measure, on top of the definitions in \eqref{eq:aceFunctions} we limit the function applied on the response variable to be a monotone function.\footnote{Without loss of generality, we can restrict it to monotonically increasing functions.} It can be readily verified that the resulting set of functions is a closed convex set. To be more concrete, we denote with $\mathcal{H}_2^m(Y)$ the set of all admissible and monotone functions $g$ such that
% \begin{align}
%     \mathbb{E}[g(Y)]&=0,~~\mathbb{E}[g^2(Y)]\leq\infty.
% \end{align}

% \begin{remark}
% Since our motivation is to be able to estimate $Y$ from $X$, a better requirement from $g$ was to be invertible (or equivalently \emph{strictly} monotone). As can be easily verified, limiting $g$ to be strictly monotone does not result with a closed set, hence to show the convergence of the algorithm we limit it to be only monotone. This suggests that if the function resulting from the algorithm is strictly monotone, it can be used as-is for the estimation process. In case this is not the case, the function can be transformed to strictly monotone for the estimation process albeit with some loss as demonstrated later.
% \end{remark}

Denoting by $P_{\mathcal{A}}(Y)$  the orthogonal projection of $Y$ onto the closed convex set $\mathcal{A}$,\footnote{Note that $\mathcal{P}_{\mathcal{H}_2(X)}\left(g(Y)\right)=\mathbb{E}\left[ g(Y)\given X \right]$.} the modified ACE algorithm is described in Algorithm~\ref{Alg:ACE_Single_Predictor}.
\begin{algorithm}
\caption{Modified ACE single predictor}\label{Alg:ACE_Single_Predictor}
\begin{algorithmic}[1]
\Procedure{CalculateSemiMonotoneMeasure}{}
\State Set $g(Y)=Y/\|Y\|$;
\While{$e^2(g,f)$ decreases}
    % \While{$e^2(g,f_1,\ldots,f_p)$ decreases}
        % \For{\texttt{$k=1$ to $p$}}
            %\State $f'_{k}(x_k)=\mathbb{E}[f(y)-\sum_{i\neq k}f_i(x_i)|x_k]$
            \State $f'(X)=\mathcal{P}_{\mathcal{H}_2(X)}\left(g(Y)\right)$
            %\State $f'(X)=f'(X)/\|f'(X)\|$
            \State replace $f(X)$ with $f'(X)/\|f'(X)\|$
        % \EndFor
    % \EndWhile
    \State $g'(Y)=\mathcal{P}_{\mathcal{M_{\kappa}}(Y)}\left(f(X)\right)$
    % \State $g'(Y)=g'(Y)/\|g'(Y)\|$
    \State replace $g(Y)$ with $g'(Y)/\|g'(Y)\|$
\EndWhile
\State End modified ACE
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{theorem}
The Pearson correlation coefficient corresponding to the sequence of  function pairs, $(f^{(n)},g^{(n)})$, defined by the modified ACE algorithm, converges to the semi-$\kappa$-monotone maximal correlation measure.
\end{theorem}
\begin{proof}
To show convergence, we note that the ACE algorithm is  an alternating minimization algorithm. This class of algorithms was suggested in \cite{cheney1959proximity} and extended in \cite{csiszar1984information}; see also \cite{byrne2011alternating}.

It is shown in \cite{csiszar1984information}   that if $P$ and $Q$ are closed convex subsets of a Hilbert space, alternating minimization converges to the global minimum.
%When analyzing the modified ACE algorithm,
%
As $\mathcal{M}_{\kappa}(Y)$ and $\mathcal{H}_2(X)$ satisfy these conditions,
%and the inner since it can be shown that the inner product $<f(X)',f(X)>=\mathbb{E}[f'(X)\cdot f(X)]$ is equivalent to the euclidean norm,
the claim follows.
\end{proof}

\subsection{Multi-variate predictor}

In the case of a multi-variate predictor, the ACE algorithm seeks an optimal linear regression between a transformation of $Y$ and a component-wise non-linear transformation of the predictor random vector $\bf{X}$. The latter transformations are defined by a set of admissible functions $f_1,\ldots,f_p$, each function operating on the corresponding random variable, yielding an estimator of the form $\sum_i f_i(X_i)$.
% , such that
% \begin{align}
%     \mathbb{E}[f_j(X_j)]=0~~j=1,\ldots,p \nonumber \\
%     \mathbb{E}[f_j^2(X_j)]\leq\infty~~j=1,\ldots,p.
%     \label{eq:aceFunctionsMulti}
% \end{align}
% Therefore, we define with $\mathcal{H}_2(X_i)$ the subspace of $\mathcal{H}_2$ which is the set of all random variables that correspond to an admissible function of $X_i$.

%Hilbert space with the inner product $<f_i(X_i)',f_i(X_i)>=\mathbb{E}[f_i'(X_i)\cdot f_i(X_i)]$.

The modified ACE algorithm for the case of a multi-variate predictor, restricting $g$ to be $\kappa$-increasing, is a simple extension of Algorithm~\ref{Alg:ACE_Single_Predictor} and can be found in \cite{semiEpsMonoFullPaper:Domanovitz2019}.

% The modified ACE algorithm for the case of a multi-variate predictor, resticting $g$ to be $\kappa$-increasing,  is described in Algorithm~\ref{Alg:ACE_Multi_Predictor}.
% \begin{algorithm}
% \caption{Modified multi-variate ACE}\label{Alg:ACE_Multi_Predictor}
% \begin{algorithmic}[1]
% \Procedure{CalculateSemiMonotoneMeasure}{}
% \State Set $g(Y)=Y/\|Y\|$ and $f_1(x_1),\cdots,f_p(x_p)=0$;
% \While{$e^2(g,f_1,\ldots,f_p)$ decreases}
%     \While{$e^2(g,f_1,\ldots,f_p)$ decreases}
%         \For{\texttt{$k=1$ to $p$}}
%             %\State $f'_{k}(x_k)=\mathbb{E}[f(y)-\sum_{i\neq k}f_i(x_i)|x_k]$
%             \State $f'_{k}(X_k)=$ \newline
%             \hspace*{6em} $\mathcal{P}_{\mathcal{H}_2(X_k)}\left(g(Y)-\sum_{i\neq k}f_{i}(X_i)\right)$
%             \State replace $f_k(X_k)$ with $f'_{k}(X_k)/\|f'_{k}(X_k)\|$
%         \EndFor
%     \EndWhile
%     \State $g'(Y)=\mathcal{P}_{\mathcal{M_{\kappa}}(Y)}(Y)\left(\sum_i f_{i}(X_i) \right)$
%     %\State $g'(Y)=g'(Y)/\|g'(Y)\|$
%     \State replace $g(Y)$ with $g'(Y)/\|g'(Y)\|$
% \EndWhile
% \State End modified ACE
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}


% We start by noting the that ACE algorithm is actually an alternating minimization algorithm. This class of algorithms was suggested in \cite{csiszar1984information}. See also \cite{byrne2011alternating}. In \cite{csiszar1984information} it was shown that if $P$ and $Q$ are closed convex subsets of Hilbert space with euclidean distance, alternating minimization converges to the global minimum.





% In our modified algorithm, we alternate between these spaces and the set of monotone functions defined on the random variable $Y$ such that
% \begin{align}
%     \mathbb{E}(g(Y))&=0,~~\mathbb{E}(g^2(y))&\leq\infty.
% \end{align}
% We denote this set as $\mathcal{H}_2^m(y)$. It can be readily shown that this set is closed and convex. Since the monotone functions with finite variable defined on random variable $Y$ are sub set of all functions with finite variance defined on random variable $Y$ we conclude that $\mathcal{H}_2^m(y)$ is closed convex subsets of Hilbert space.




%\section{Relation to Jeffrey's Prior}

\section{Numerical example}
\label{sec:numericalExample}
\ifdefined\EXAMPLES

In this section we give examples in which the semi-$\kappa$-monotone maximal correlation measure (evaluated using the modified ACE algorithm) results in a significant improvement over the  standard maximal correlation measure (evaluated using the standard ACE algorithm) in the context of  estimation of a random variable $Y$ from a random vector $\bf{X}$. We further demonstrate its potential for improvement over the correlation ratio.

We begin with a multi-variate  example where one of the two observed random variables ``masks'' the other while the latter is more significant for estimation purposes. In the second example we demonstrate why it is not sufficient to restrict $g(Y)$ to be monotone (or equivalently, to set $\kappa=0$). The third example illustrates why the semi-$\kappa$-monotone maximal correlation measure may be advantageous with respect to  the correlation ratio.

For simulating ACE, we used the ACE Matlab code provided by the authors of \cite{voss1997reconstruction}. To limit $g$ to be an $\kappa$-monotonic function we used isotonic regression followed by a regularization which is described in more detail below.

\subsection{Example 1 - Multi-variate predictor}

Assume that the response variable $Y$ is distributed uniformly over the interval $[0,1]$. Assume we have two predictor variables
\begin{align}
    X_1&=\rm{mod}(Y,0.2)+N_1 \nonumber \\
    X_2&=Y^3+N_2
    \label{eq:example1}
\end{align}
where $N_1,N_2$ are independent zero-mean Gaussian variables with  $\sigma^2_{N_1}=0.01$ and $\sigma^2_{N_2}=0.2$.

Calculating the maximal correlation coefficient results in ``shadowing'' the more significant variable ($X_2$) for estimation purposes of the response $Y$. To see this, we start by running the ACE algorithm to evaluate the maximal correlation coefficient between $Y$ and $X_1$. As can be seen from Figure~\ref{fig:y_x1_ACE}, this results in a very high value. Inspecting the transformations yielding this result, we note that $g$ is not monotonic and hence we cannot recover $Y$ from $g(Y)$. %Hence, after applying it on $Y$, we %can't reconstruct the original data. In this example, there is a small fraction of the data (similar to example described above) which is common to both variables. Hence, while applying functions which disregard the rest of the data, high correlation can be attained.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x1_ACE.eps}
    \caption{Example 1: Running ACE on $Y$ and $X_1$.}
    \label{fig:y_x1_ACE}
\end{figure}

Next, we apply the ACE algorithm to calculate the maximal correlation coefficient between $Y$ and $X_2$. As can be seen from Figure~\ref{fig:y_x2_ACE},  this value is much smaller (than that between $Y$ and $X_1$) since in this case we have stronger additive noise. Nevertheless, the transformation applied to $Y$ is now monotonic. Therefore, even though the maximal correlation coefficient is smaller, the observation $X_2$ can better serve for estimation of $Y$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x2_ACE.eps}
    \caption{Example 1: Running ACE on $Y$ and $X_2$.}
    \label{fig:y_x2_ACE}
\end{figure}

Next we apply the ACE algorithm to %obtain the maximal correlation coefficient between
$Y$ and the
vector $(X_1,X_2)$. As can be seen from Figure~\ref{fig:y_x1_x2_ACE}, the ACE algorithm, in order to maximize the  correlation, chooses similar functions as in case of running only on $Y$ and $X_1$,  practically choosing to ignore $X_2$. While, indeed, this maximizes the correlation coefficient, it is far from satisfying from an estimation viewpoint.
% results, again, with an outcome which is useless when estimation of $Y$ from $X_1$ and $X_2$ is sought.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x1_x2_ACE.eps}
    \caption{Example 1: Running ACE on $Y$, $X_1$ and $X_2$.}
    \label{fig:y_x1_x2_ACE}
\end{figure}

We now observe that the modified ACE algorithm does not suffer from this deficiency. To obtain the orthogonal projection onto the subset of $\kappa$-increasing functions $\mathcal{M_{\kappa}}(Y)$, we use isotonic regression followed by the following regularization\footnote{Note that this method of regularization actually forces the minimal slope to be slightly larger than $\kappa$.}
\begin{align}
    g(Y)=g(Y)+\kappa\cdot Y.
\end{align}
As can be seen from Figure~\ref{fig:y_x1_x2_SemiMon_stric},
%restricting $g$ to be a monotone %function and forcing a slope of at %least
setting $\kappa=0.1$, the resulting value of the semi-$\kappa$-monotone maximal correlation measure is very close to the maximal correlation value between $Y$ and $X_2$. Thus, the algorithm ``chooses to ignore'' $X_1$ (even though it suffers from a lower noise level) and bases the estimation on $X_2$.
%%chose the variable which is more %meaningful for estimation purposes.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\figSize\columnwidth]{y_x1_x2_SemiMon.eps}
%     \caption{Running modified ACE on $Y$, $X_1$ and $X_2$.}
%     \label{fig:y_x1_x2_SemiMon}
% \end{figure}

% In case the algorithm results with a strictly monotone function, this function is invertible and hence these transformations shed light on the estimation quality of $Y$ from $X$. However, as noted above, we can prove that the ACE algorithm converges to the optimal solution while only restricting the function to be monotone (and not strictly monotone). To overcome it, when running the modified ACE, on top of performing isotonic regression we forced the resulting function to be strictly monotone (by performing linear interpolation between edge points of a plateau). As can be seen, it results with slightly lower correlation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x1_x2_SemiMon_stric_2.eps}
    \caption{Example 1: Running modified ACE on $Y$, $X_1$ and $X_2$ with $\kappa=0.1$.}
    \label{fig:y_x1_x2_SemiMon_stric}
\end{figure}

\subsection{Example 2 - Semi-$0$-monotonicity is insufficient}

To illustrate why it does not suffice to limit $g$ to be merely monotone, consider the following example. Assume that the response $Y$ is distributed uniformly over the interval $[-10,10]$ and that
%Assume we have the following %predictor variable
\begin{align}
    X=\begin{cases}
    X=Y~~~Y>9 \\
    X=N_1~~~ {\rm otherwise}
    \end{cases}
    \label{eq:example2}
\end{align}
where $N_1\sim {\rm Unif}([-1,1])$ and is independent of $Y$.

Limiting $g$ only to be monotone %(equivalently, limiting $g(Y)$ to be monotone without any limitation on the minimal slope),
(with no limitation on minimal slope)
results in a correlation value of $1$ %equals to one
since the optimal solution is to set $g(y)=0$ in the region it cannot be estimated %from $X$
and $g(y)=y$ otherwise (and then apply normalization). Clearly, the function $g$ is non-invertible as is depicted in Figure~\ref{fig:y_x_exmp2_onlyMono}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x_exmp2_onlyMono.eps}
    \caption{Example 2: Running modified ACE on $Y$, $X$ with $\kappa=0$.}
    \label{fig:y_x_exmp2_onlyMono}
\end{figure}

Next, we run the modified ACE algorithm, enforcing a minimal slope of  $\kappa=0.1$.
%Now, a similar solution as described above is not possible since it doesn't meet the requirement on the minimal slope of $g(Y)$.
The results are depicted in Figure~\ref{fig:y_x_exmp2_Mono_0_1}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x_exmp2_Mono_0_1.eps}.
    \caption{Example 2: Running modified ACE on $Y$, $X$ with $\kappa=0.1$}
    \label{fig:y_x_exmp2_Mono_0_1}
\end{figure}
This example sheds light on the trade-off that exists when setting the value of $\kappa$.
Setting  $\kappa$ to be large
limits the possible gain over the correlation ratio  whereas setting it too low
%
%As this value grows, the number of %potential options for $g(Y)$ gets %lower (in the extreme case of very %large $\kappa$ only $g(Y)=Y$ is %a possible solution and therefore %the semi-$\kappa$-monotone %converges to the correlation ratio). %However, setting $\kappa$ too low might suggest that the
risks overemphasizing regions where the noise is smaller.
%suffering a``penalty'' imposed in %cases similar to the example below %(where $y=f(x)$ in part of the %region) is smaller, hence these cases will get higher score and might result with misleading insights with respect to the potential of estimating $Y$ from $X$.



\subsection{Example 3 - Comparisons with correlation ratio}

The suggested measure can be viewed as a generalization of the correlation ratio (the correlation ratio amounts to setting $g$ to have a constant slope of $1$).

We first demonstrate how the semi-$\kappa$-monotone maximal correlation measure deals with a well-known example where the correlation ratio equals $0$ for a pair of dependent random variables. In this example we assume $X$ and $Y$ are uniformly distributed over a circle with radius $1$.  The correlation ratio is $0$ as depicted in Figure~\ref{fig:y_x_exmp4_corr_ratio} where we ran ACE enforcing $g(y)=y$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x_exmp4_corr_ratio.eps}.
    \caption{Example 3a: Optimal transformation corresponding to the correlation ratio.}
    \label{fig:y_x_exmp4_corr_ratio}
\end{figure}

Applying the semi-$\kappa$-monotone maximal correlation measure with $\kappa=0.1$ yields a much larger correlation. Thus, it manages to capture the dependence between $X$ and $Y$. This is depicted in Figure~\ref{fig:y_x_exmp4_semi_mono_0_1}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x_exmp4_semi_mono_0_1.eps}.
    \caption{Example 3b: Optimal transformations corresponding to the semi-$\kappa$-monotone maximal correlation measure with $\kappa=0.1$. }
    \label{fig:y_x_exmp4_semi_mono_0_1}
\end{figure}


%(in fact, the correlation ratio is exactly the semi-$1$-monotone measure).
%A natural question is why not set $\kappa=1$ and just use the correlation ratio.
% A natural question is whether the correlation ratio and correspondingly using the ACE algorithm with $g$ frozen to $g(y)=y$ is sufficient.

The next example demonstrates another potential advantage over the correlation ratio.
As was already noted, there are cases where there is no a priori preferred (natural) parameterization for the
response variable and thus choosing one that maximizes the correlation may be a reasonable approach as we now demonstrate.
%using different parameterization %improves the correlation (and thus %results with better estimation).
%We demonstrate next that using the %semi-$\kappa$-monotone measure %can yeild the benefits of %reparameterization.

Assume that the response variable $Y$ is distributed uniformly over the interval $[0,10]$ and that the predictor variable $X$ is
\begin{align}
    X=\log(Y)+N,
    \label{eq:example3}
\end{align}
where $N$ is a zero-mean Gaussian (and independent of $Y$) with unit variance. %$\sigma^2=1$.
Comparing the correlation ratio (Figure~\ref{fig:y_x_exmp3_Corr_Ratio}) to the semi-$0.1$-monotone correlation measure (Figure~\ref{fig:y_x_exmp3_semi_0_1}) reveals  that the correlation of the latter is significantly higher.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x_exmp3_Corr_Ratio.eps}.
    \caption{Example 3b: Optimal transformations corresponding to the correlation ratio.}
    \label{fig:y_x_exmp3_Corr_Ratio}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x_exmp3_semi_0_1.eps}}
    \caption{Example 3b: Optimal transformations corresponding to the semi-$0.1$-monotone correlation measure.}
    \label{fig:y_x_exmp3_semi_0_1}
\end{figure}

\else
Assume that the response variable $Y$ is distributed uniformly over the interval $[0,1]$. Assume we have two predictor variables
\begin{align}
    X_1=\rm{mod}(Y,0.2)+N_1,~X_2=Y^3+N_2
    \label{eq:example1}
\end{align}
where $N_1,N_2$ are independent zero-mean Gaussian variables with  $\sigma^2_{N_1}=0.01$ and $\sigma^2_{N_2}=0.2$.

Calculating the maximal correlation coefficient results in ``shadowing'' the more significant variable ($X_2$) for estimation purposes of the response $Y$. To see this, we start by running the ACE algorithm to evaluate the maximal correlation coefficient between $Y$ and $X_1$. For simulating ACE, we used the ACE Matlab code provided by the authors of \cite{voss1997reconstruction}. %To limit $g$ to be an $\kappa$-monotonic function we used isotonic regression followed by a regularization which is described in more detail below.

As can be seen from Figure~\ref{fig:y_x1_ACE}, this results in a very high value. Inspecting the transformations yielding this result, we note that $g$ is not monotonic and hence we cannot recover $Y$ from $g(Y)$. %Hence, after applying it on $Y$, we %can't reconstruct the original data. In this example, there is a small fraction of the data (similar to example described above) which is common to both variables. Hence, while applying functions which disregard the rest of the data, high correlation can be attained.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x1_ACE_shrink.eps}
    \caption{Example 1: Running ACE on $Y$ and $X_1$.}
    \label{fig:y_x1_ACE}
\end{figure}

Next, we apply the ACE algorithm to calculate the maximal correlation coefficient between $Y$ and $X_2$. As can be seen from Figure~\ref{fig:y_x2_ACE},  this value is much smaller (than that between $Y$ and $X_1$) since in this case we have stronger additive noise. Nevertheless, the transformation applied to $Y$ is now monotonic. Therefore, even though the maximal correlation coefficient is smaller, the observation $X_2$ can better serve for estimation of $Y$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x2_ACE_shrink.eps}
    \caption{Example 1: Running ACE on $Y$ and $X_2$.}
    \label{fig:y_x2_ACE}
\end{figure}

Next we apply the ACE algorithm to %obtain the maximal correlation coefficient between
$Y$ and the
vector $(X_1,X_2)$. As can be seen from Figure~\ref{fig:y_x1_x2_ACE}, the ACE algorithm, in order to maximize the  correlation, chooses similar functions as in case of running only on $Y$ and $X_1$,  practically choosing to ignore $X_2$. While, indeed, this maximizes the correlation coefficient, it is far from satisfying from an estimation viewpoint.
% results, again, with an outcome which is useless when estimation of $Y$ from $X_1$ and $X_2$ is sought.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x1_x2_ACE.eps}
    \caption{Example 1: Running ACE on $Y$, $X_1$ and $X_2$.}
    \label{fig:y_x1_x2_ACE}
\end{figure}

We now observe that the modified ACE algorithm does not suffer from this deficiency. To obtain the orthogonal projection onto the subset of $\kappa$-increasing functions $\mathcal{M_{\kappa}}(Y)$, we use isotonic regression followed by the following regularization\footnote{Note that this method of regularization actually forces the minimal slope to be slightly larger than $\kappa$.}
\begin{align}
    g(Y)=g(Y)+\kappa\cdot Y.
\end{align}
As can be seen from Figure~\ref{fig:y_x1_x2_SemiMon_stric},
%restricting $g$ to be a monotone %function and forcing a slope of at %least
setting $\kappa=0.1$, the resulting value of the semi-$\kappa$-monotone maximal correlation measure is very close to the maximal correlation value between $Y$ and $X_2$. Thus, the algorithm ``chooses to ignore'' $X_1$ (even though it suffers from a lower noise level) and bases the estimation on $X_2$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\figSize\columnwidth]{y_x1_x2_SemiMon_stric_2.eps}
    \caption{Example 1: Running modified ACE on $Y$, $X_1$ and $X_2$ with $\kappa=0.1$.}
    \label{fig:y_x1_x2_SemiMon_stric}
\end{figure}
\fi

% \begin{table}[]
% \begin{tabular}{|l|l|l|l|}
% \hline
%         & Renyi                                                                                                                                                                                 & Hall                                                                                                                                                                                                                                                                                                       & Hiu                                                                                                                                                                        \\ \hline
% Axiom A & \begin{tabular}[c]{@{}l@{}}$\rho$ is defined for any \\ non-trivial X and Y\end{tabular}                                                                                             & \begin{tabular}[c]{@{}l@{}}$\delta$ is defined \\ for any (X,Y)\end{tabular}                                                                                                                                                                                                                               & \begin{tabular}[c]{@{}l@{}}$D(x,y)$ is defined for all\\  continuous random variables \\ X,Y\end{tabular}                                                                  \\ \hline
% Axiom B & $\rho(X,Y)=\rho(Y,X)$                                                                                                                                                                 & DISCARD                                                                                                                                                                                                                                                                                                    & DISCARD                                                                                                                                                                    \\ \hline
% Axiom C & $0\leq\rho\leq 1$                                                                                                                                                                     & UNALTERED                                                                                                                                                                                                                                                                                                  & UNALTERED                                                                                                                                                                  \\ \hline
% Axiom D & \begin{tabular}[c]{@{}l@{}}$\rho=0$ iff X and Y \\ are independent\end{tabular}                                                                                                       & UNALTERED                                                                                                                                                                                                                                                                                                  & UNALTERED                                                                                                                                                                  \\ \hline
% Axiom \mathbb{E} & \begin{tabular}[c]{@{}l@{}}$\rho =1$ if either \\ $X =g(Y)$ a.s. or \\ $Y =f(X)$  a.s. for some \\ measurable functions \\ f and g\end{tabular}                                       & \begin{tabular}[c]{@{}l@{}}$\delta=1$ if there exists \\ a r.v. $g(Y)$ for which \\ $X = g(Y)$ a.s\end{tabular}                                                                                                                                                                                            & \begin{tabular}[c]{@{}l@{}}$D(X,Y)= 1$ if and only if \\ $Y=f(X)$ almost surely for\\ a Borel-measurable function $f$\end{tabular}                                         \\ \hline
% Axiom F & \begin{tabular}[c]{@{}l@{}}If f and g are measurable \\ functions then \\ $\rho(f(X), g(Y))\leq\rho(X,Y)$, \\ with equality holding \\ if both f and g are 1-1\end{tabular}           & \begin{tabular}[c]{@{}l@{}}(a) If g is a measurable \\ function then \\ $\delta(X,g(Y) ) \leq\delta(X,Y)$,\\  with equality holding if \\ g is 1-1.\\ (b) $\delta(aX+bY) = \delta(X,Y)$\\  for arbitrary real a and b ($a\neqb$).\end{tabular}                                                             & \begin{tabular}[c]{@{}l@{}}If $g$ is a Borel-measurable\\  bijection on $\mathcal{R}$, \\ then $D(g(X),Y)=D(X,Y)$\end{tabular}                                             \\ \hline
% Axiom G & \begin{tabular}[c]{@{}l@{}}If $(X,Y)$ are bivariate normal \\ in distribution, then \\ $\rho(X,Y)$ is the absolute \\ value of the correlation \\ coefficient of $(X,Y)$\end{tabular} & \begin{tabular}[c]{@{}l@{}}If $Z_1,Z_2,\cdots$ are independent \\ and identically distributed \\ non-trivial r.v.'s, and \\ $Y = Z1+ \cdots +Z_k$ and \\ $X = Z_{k+1} + \cdots + Z_{k+l}$\\  for arbitrary non-negative\\ integers k and l \\ $(k+ l > 0)$, then \\ $\delta(X,Y) = k/(k+l )$.\end{tabular} & \begin{tabular}[c]{@{}l@{}}If $X,Y$ are jointly normal with\\  correlation coefficient $\rho$, \\ then $D(X,Y)$ is a strictly\\ increasing function of $\rho$\end{tabular} \\ \hline
% \end{tabular}
% \end{table}



\bibliographystyle{IEEEtran}
\bibliography{eladd}

% \begin{IEEEbiographynophoto}{Elad Domanovitz}
%  received the B.Sc. degree (cum laude) and the M.Sc. degree in 2005 and 2011, respectively, in electrical engineering
%  from Tel Aviv University, Israel. He is currently working toward the Ph.D. degree at Tel Aviv University.
%  \end{IEEEbiographynophoto}
%  %\vspace{-14 mm}
%  \begin{IEEEbiographynophoto}{Uri Erez}
%  (M'09) was born in Tel-Aviv, Israel, on October 27, 1971.
%  He received the B.Sc. degree in mathematics and physics and the M.Sc. and Ph.D. degrees in electrical engineering from Tel-Aviv University in 1996, 1999, and 2003, respectively. During 2003-2004, he was a Postdoctoral
%  Associate at the Signals, Information and Algorithms Laboratory at the Massachusetts Institute of Technology (MIT), Cambridge. Since 2005, he has been with the Department of Electrical Engineering-Systems at Tel-Aviv
%  University. His research interests are in the general areas of information theory and digital communication. He served in the years 2009-2011 as Associate Editor for Coding Techniques for the IEEE TRANSACTIONS ON INFORMATION THEORY.\end{IEEEbiographynophoto}


\end{document}